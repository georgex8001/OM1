# OM1 - Modular AI Runtime for Robots

<div align="center">

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡) | [Tiáº¿ng Viá»‡t](#tiáº¿ng-viá»‡t)

[![Technical Paper](https://img.shields.io/badge/Technical-Paper-blue)](https://openmind.org) 
[![Documentation](https://img.shields.io/badge/Docs-docs.openmind.org-green)](https://docs.openmind.org)
[![Discord](https://img.shields.io/badge/Discord-Join-purple)](https://discord.gg/openmind)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

---

<a name="english"></a>

![OM_Banner_X2](https://user-images.githubusercontent.com/placeholder/om_banner.png)

**[Technical Paper](https://openmind.org)** | **[Documentation](https://docs.openmind.org)** | **[X/Twitter](https://x.com/openmind_agi)** | **[Discord](https://discord.gg/openmind)**

**OpenMind's OM1 is a modular AI runtime that empowers developers to create and deploy multimodal AI agents across digital environments and physical robots**, including Humanoids, Phone Apps, websites, Quadrupeds, and educational robots such as TurtleBot 4. OM1 agents can process diverse inputs like web data, social media, camera feeds, and LIDAR, while enabling physical actions including motion, autonomous navigation, and natural conversations. The goal of OM1 is to make it easy to create highly capable human-focused robots, that are easy to upgrade and (re)configure to accommodate different physical form factors.

## Capabilities of OM1

* **Modular Architecture**: Designed with Python for simplicity and seamless integration.
* **Data Input**: Easily handles new data and sensors.
* **Hardware Support via Plugins**: Supports new hardware through plugins for API endpoints and specific robot hardware connections to `ROS2`, `Zenoh`, and `CycloneDDS`. (We recommend `Zenoh` for all new development).
* **Web-Based Debugging Display**: Monitor the system in action with WebSim (available at http://localhost:8000/) for easy visual debugging.
* **Pre-configured Endpoints**: Supports Voice-to-Speech, OpenAI's `gpt-4o`, DeepSeek, and multiple Visual Language Models (VLMs) with pre-configured endpoints for each service.

## Architecture Overview

![Architecture](https://user-images.githubusercontent.com/placeholder/architecture.png)

## Getting Started - Hello World

To get started with OM1, let's run the Spot agent. Spot uses your webcam to capture and label objects. These text captions are then sent to `OpenAI 4o`, which returns `movement`, `speech` and `face` action commands. These commands are displayed on WebSim along with basic timing and other debugging information.

### Package Management and VENV

You will need the uv package manager.

### Clone the Repo

```bash
git clone https://github.com/openmind/OM1.git
cd OM1
git submodule update --init
uv venv
```

### Install Dependencies

**For MacOS:**

```bash
brew install portaudio ffmpeg
```

**For Linux:**

```bash
sudo apt-get update
sudo apt-get install portaudio19-dev python-dev ffmpeg
```

### Obtain an OpenMind API Key

Obtain your API Key at [OpenMind Portal](https://portal.openmind.org). Copy it to `config/spot.json5`, replacing the `openmind_free` placeholder. Or, `cp env.example .env` and add your key to the `.env`.

### Launching OM1

```bash
uv run src/run.py spot
```

After launching OM1, the Spot agent will interact with you and perform (simulated) actions. For more help connecting OM1 to your robot hardware, see [getting started](https://docs.openmind.org/getting-started).

## What's Next?

* Try out some [examples](https://docs.openmind.org/examples)
* Add new `inputs` and `actions`.
* Design custom agents and robots by creating your own `json5` config files with custom combinations of inputs and actions.
* Change the system prompts in the configuration files (located in `/config/`) to create new behaviors.

## Interfacing with New Robot Hardware

OM1 assumes that robot hardware provides a high-level SDK that accepts elemental movement and action commands such as `backflip`, `run`, `gently pick up the red apple`, `move(0.37, 0, 0)`, and `smile`. An example is provided in `actions/move_safe/connector/ros2.py`:

```python
...
elif output_interface.action == "shake paw":
    if self.sport_client:
        self.sport_client.Hello()
...
```

If your robot hardware does not yet provide a suitable HAL (hardware abstraction layer), traditional robotics approaches such as RL (reinforcement learning) in concert with suitable simulation environments (Unity, Gazebo), sensors (such as hand mounted ZED depth cameras), and custom VLAs will be needed for you to create one. It is further assumed that your HAL accepts motion trajectories, provides battery and thermal management/monitoring, and calibrates and tunes sensors such as IMUs, LIDARs, and magnetometers.

OM1 can interface with your HAL via USB, serial, ROS2, CycloneDDS, Zenoh, or websockets. For an example of an advanced humanoid HAL, please see [Unitree's C++ SDK](https://github.com/unitreerobotics). Frequently, a HAL, especially ROS2 code, will be dockerized and can then interface with OM1 through DDS middleware or websockets.

## Recommended Development Platforms

OM1 is developed on:

* Jetson AGX Orin 64GB (running Ubuntu 22.04 and JetPack 6.1)
* Mac Studio with Apple M2 Ultra with 48 GB unified memory (running MacOS Sequoia)
* Mac Mini with Apple M4 Pro with 48 GB unified memory (running MacOS Sequoia)
* Generic Linux machines (running Ubuntu 22.04)

OM1 _should_ run on other platforms (such as Windows) and microcontrollers such as the Raspberry Pi 5 16GB.

## Full Autonomy Guidance

We're excited to introduce **full autonomy mode**, where three services work together in a loop without manual intervention:

* **om1**
* **unitree_go2_ros2_sdk** â€“ A ROS 2 package that provides SLAM (Simultaneous Localization and Mapping) capabilities for the Unitree Go2 robot using an RPLiDAR sensor, the SLAM Toolbox and the Nav2 stack.
* **om1-avatar** â€“ A modern React-based frontend application that provides the user interface and avatar display system for OM1 robotics software.

## Intro to Backpack

From research to real-world autonomy, a platform that learns, moves, and builds with you. We'll shortly be releasing the **BOM** and details on **DIY** for it. Stay tuned!

Clone the following repos:

* https://github.com/OpenMind/OM1.git
* https://github.com/OpenMind/unitree_go2_ros2_sdk.git
* https://github.com/OpenMind/OM1-avatar.git

## Starting the System

To start all services, run the following commands:

### For OM1

Setup the API key:

For Bash: `vim ~/.bashrc` or `~/.bash_profile`.

For Zsh: `vim ~/.zshrc`.

Add:

```bash
export OM_API_KEY="your_api_key"
```

Update the docker-compose file. Replace "unitree_go2_autonomy_advance" with the agent you want to run:

```yaml
command: ["unitree_go2_autonomy_advance"]
```

```bash
cd OM1
docker-compose up om1 -d --no-build
```

### For unitree_go2_ros2_sdk

```bash
cd unitree_go2_ros2_sdk
docker-compose up orchestrator -d --no-build
docker-compose up om1_sensor -d --no-build
docker-compose up watchdog -d --no-build
```

### For OM1-avatar

```bash
cd OM1-avatar
docker-compose up om1_avatar -d --no-build
```

## Detailed Documentation

More detailed documentation can be accessed at [docs.openmind.org](https://docs.openmind.org).

## Contributing

Please make sure to read the [Contributing Guide](CONTRIBUTING.md) before making a pull request.

## License

This project is licensed under the terms of the [MIT License](LICENSE), which is a permissive free software license that allows users to freely use, modify, and distribute the software. The MIT License is a widely used and well-established license that is known for its simplicity and flexibility. By using the MIT License, this project aims to encourage collaboration, modification, and distribution of the software.

---

<a name="ä¸­æ–‡"></a>

# OM1 - æ¨¡å—åŒ–æœºå™¨äººAIè¿è¡Œæ—¶

**[English](#english)** | **ä¸­æ–‡** | **[Tiáº¿ng Viá»‡t](#tiáº¿ng-viá»‡t)**

![OM_Banner_X2](https://user-images.githubusercontent.com/placeholder/om_banner.png)

**[æŠ€æœ¯è®ºæ–‡](https://openmind.org)** | **[æ–‡æ¡£](https://docs.openmind.org)** | **[X/Twitter](https://x.com/openmind_agi)** | **[Discord](https://discord.gg/openmind)**

**OpenMind çš„ OM1 æ˜¯ä¸€ä¸ªæ¨¡å—åŒ– AI è¿è¡Œæ—¶ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿåœ¨æ•°å­—ç¯å¢ƒå’Œç‰©ç†æœºå™¨äººä¸Šåˆ›å»ºå’Œéƒ¨ç½²å¤šæ¨¡æ€ AI æ™ºèƒ½ä½“**ï¼ŒåŒ…æ‹¬äººå½¢æœºå™¨äººã€æ‰‹æœºåº”ç”¨ã€ç½‘ç«™ã€å››è¶³æœºå™¨äººä»¥åŠ TurtleBot 4 ç­‰æ•™è‚²æœºå™¨äººã€‚OM1 æ™ºèƒ½ä½“å¯ä»¥å¤„ç†ç½‘ç»œæ•°æ®ã€ç¤¾äº¤åª’ä½“ã€æ‘„åƒå¤´ç”»é¢å’Œæ¿€å…‰é›·è¾¾ç­‰å¤šæ ·åŒ–è¾“å…¥ï¼ŒåŒæ—¶æ”¯æŒè¿åŠ¨ã€è‡ªä¸»å¯¼èˆªå’Œè‡ªç„¶å¯¹è¯ç­‰ç‰©ç†åŠ¨ä½œã€‚OM1 çš„ç›®æ ‡æ˜¯è®©åˆ›å»ºé«˜æ€§èƒ½ã€ä»¥äººä¸ºæœ¬çš„æœºå™¨äººå˜å¾—ç®€å•ï¼Œå¹¶ä¸”æ˜“äºå‡çº§å’Œï¼ˆé‡æ–°ï¼‰é…ç½®ä»¥é€‚åº”ä¸åŒçš„ç‰©ç†å½¢æ€ã€‚

## OM1 çš„èƒ½åŠ›

* **æ¨¡å—åŒ–æ¶æ„**ï¼šé‡‡ç”¨ Python è®¾è®¡ï¼Œç®€å•ä¸”æ— ç¼é›†æˆã€‚
* **æ•°æ®è¾“å…¥**ï¼šè½»æ¾å¤„ç†æ–°æ•°æ®å’Œä¼ æ„Ÿå™¨ã€‚
* **é€šè¿‡æ’ä»¶æ”¯æŒç¡¬ä»¶**ï¼šé€šè¿‡æ’ä»¶æ”¯æŒæ–°ç¡¬ä»¶ï¼ŒåŒ…æ‹¬ API ç«¯ç‚¹ä»¥åŠä¸ `ROS2`ã€`Zenoh` å’Œ `CycloneDDS` çš„ç‰¹å®šæœºå™¨äººç¡¬ä»¶è¿æ¥ã€‚ï¼ˆæˆ‘ä»¬å»ºè®®æ‰€æœ‰æ–°å¼€å‘ä½¿ç”¨ `Zenoh`ï¼‰ã€‚
* **åŸºäº Web çš„è°ƒè¯•æ˜¾ç¤º**ï¼šé€šè¿‡ WebSimï¼ˆå¯åœ¨ http://localhost:8000/ è®¿é—®ï¼‰ç›‘æ§ç³»ç»Ÿè¿è¡Œï¼Œä¾¿äºå¯è§†åŒ–è°ƒè¯•ã€‚
* **é¢„é…ç½®ç«¯ç‚¹**ï¼šæ”¯æŒè¯­éŸ³è½¬æ–‡æœ¬ã€OpenAI çš„ `gpt-4o`ã€DeepSeek ä»¥åŠå¤šä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œæ¯ä¸ªæœåŠ¡éƒ½æœ‰é¢„é…ç½®ç«¯ç‚¹ã€‚

## æ¶æ„æ¦‚è¿°

![æ¶æ„å›¾](https://user-images.githubusercontent.com/placeholder/architecture.png)

## å…¥é—¨ - Hello World

è¦å¼€å§‹ä½¿ç”¨ OM1ï¼Œè®©æˆ‘ä»¬è¿è¡Œ Spot æ™ºèƒ½ä½“ã€‚Spot ä½¿ç”¨æ‚¨çš„ç½‘ç»œæ‘„åƒå¤´æ•è·å’Œæ ‡è®°ç‰©ä½“ã€‚è¿™äº›æ–‡æœ¬æè¿°éšåå‘é€åˆ° `OpenAI 4o`ï¼Œåè€…è¿”å› `è¿åŠ¨`ã€`è¯­éŸ³` å’Œ `è¡¨æƒ…` åŠ¨ä½œå‘½ä»¤ã€‚è¿™äº›å‘½ä»¤ä¼šåœ¨ WebSim ä¸Šæ˜¾ç¤ºï¼ŒåŒæ—¶æ˜¾ç¤ºåŸºæœ¬çš„æ—¶é—´å’Œå…¶ä»–è°ƒè¯•ä¿¡æ¯ã€‚

### åŒ…ç®¡ç†å’Œè™šæ‹Ÿç¯å¢ƒ

æ‚¨éœ€è¦ uv åŒ…ç®¡ç†å™¨ã€‚

### å…‹éš†ä»“åº“

```bash
git clone https://github.com/openmind/OM1.git
cd OM1
git submodule update --init
uv venv
```

### å®‰è£…ä¾èµ–

**MacOSï¼š**

```bash
brew install portaudio ffmpeg
```

**Linuxï¼š**

```bash
sudo apt-get update
sudo apt-get install portaudio19-dev python-dev ffmpeg
```

### è·å– OpenMind API å¯†é’¥

åœ¨ [OpenMind Portal](https://portal.openmind.org) è·å–æ‚¨çš„ API å¯†é’¥ã€‚å°†å…¶å¤åˆ¶åˆ° `config/spot.json5`ï¼Œæ›¿æ¢ `openmind_free` å ä½ç¬¦ã€‚æˆ–è€…æ‰§è¡Œ `cp env.example .env` å¹¶å°†å¯†é’¥æ·»åŠ åˆ° `.env` æ–‡ä»¶ä¸­ã€‚

### å¯åŠ¨ OM1

```bash
uv run src/run.py spot
```

å¯åŠ¨ OM1 åï¼ŒSpot æ™ºèƒ½ä½“å°†ä¸æ‚¨äº’åŠ¨å¹¶æ‰§è¡Œï¼ˆæ¨¡æ‹Ÿï¼‰åŠ¨ä½œã€‚æœ‰å…³å°† OM1 è¿æ¥åˆ°æœºå™¨äººç¡¬ä»¶çš„æ›´å¤šå¸®åŠ©ï¼Œè¯·å‚é˜…[å…¥é—¨æŒ‡å—](https://docs.openmind.org/getting-started)ã€‚

## ä¸‹ä¸€æ­¥åšä»€ä¹ˆï¼Ÿ

* å°è¯•ä¸€äº›[ç¤ºä¾‹](https://docs.openmind.org/examples)
* æ·»åŠ æ–°çš„ `è¾“å…¥` å’Œ `åŠ¨ä½œ`ã€‚
* é€šè¿‡åˆ›å»ºè‡ªå·±çš„ `json5` é…ç½®æ–‡ä»¶æ¥è®¾è®¡å®šåˆ¶æ™ºèƒ½ä½“å’Œæœºå™¨äººï¼Œä½¿ç”¨è¾“å…¥å’ŒåŠ¨ä½œçš„è‡ªå®šä¹‰ç»„åˆã€‚
* ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼ˆä½äº `/config/` ç›®å½•ï¼‰ä¸­çš„ç³»ç»Ÿæç¤ºè¯ä»¥åˆ›å»ºæ–°è¡Œä¸ºã€‚

## ä¸æ–°æœºå™¨äººç¡¬ä»¶å¯¹æ¥

OM1 å‡è®¾æœºå™¨äººç¡¬ä»¶æä¾›é«˜çº§ SDKï¼Œå¯æ¥å—åŸºæœ¬è¿åŠ¨å’ŒåŠ¨ä½œå‘½ä»¤ï¼Œå¦‚ `åç©ºç¿»`ã€`å¥”è·‘`ã€`è½»è½»æ‹¿èµ·çº¢è‹¹æœ`ã€`move(0.37, 0, 0)` å’Œ `å¾®ç¬‘`ã€‚`actions/move_safe/connector/ros2.py` ä¸­æä¾›äº†ä¸€ä¸ªç¤ºä¾‹ï¼š

```python
...
elif output_interface.action == "shake paw":
    if self.sport_client:
        self.sport_client.Hello()
...
```

å¦‚æœæ‚¨çš„æœºå™¨äººç¡¬ä»¶å°šæœªæä¾›åˆé€‚çš„ HALï¼ˆç¡¬ä»¶æŠ½è±¡å±‚ï¼‰ï¼Œåˆ™éœ€è¦ä½¿ç”¨ä¼ ç»Ÿæœºå™¨äººæ–¹æ³•ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œé…åˆåˆé€‚çš„ä»¿çœŸç¯å¢ƒï¼ˆUnityã€Gazeboï¼‰ã€ä¼ æ„Ÿå™¨ï¼ˆå¦‚æ‰‹éƒ¨å®‰è£…çš„ ZED æ·±åº¦ç›¸æœºï¼‰å’Œè‡ªå®šä¹‰è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰æ¥åˆ›å»ºã€‚æ­¤å¤–ï¼Œå‡è®¾æ‚¨çš„ HAL æ¥å—è¿åŠ¨è½¨è¿¹ï¼Œæä¾›ç”µæ± å’Œçƒ­ç®¡ç†/ç›‘æ§ï¼Œå¹¶æ ¡å‡†å’Œè°ƒæ•´ IMUã€æ¿€å…‰é›·è¾¾å’Œç£åŠ›è®¡ç­‰ä¼ æ„Ÿå™¨ã€‚

OM1 å¯ä»¥é€šè¿‡ USBã€ä¸²å£ã€ROS2ã€CycloneDDSã€Zenoh æˆ– websockets ä¸æ‚¨çš„ HAL æ¥å£ã€‚æœ‰å…³é«˜çº§äººå½¢æœºå™¨äºº HAL çš„ç¤ºä¾‹ï¼Œè¯·å‚é˜… [Unitree çš„ C++ SDK](https://github.com/unitreerobotics)ã€‚é€šå¸¸ï¼ŒHALï¼ˆç‰¹åˆ«æ˜¯ ROS2 ä»£ç ï¼‰ä¼šè¢« Docker åŒ–ï¼Œç„¶åå¯ä»¥é€šè¿‡ DDS ä¸­é—´ä»¶æˆ– websockets ä¸ OM1 æ¥å£ã€‚

## æ¨èçš„å¼€å‘å¹³å°

OM1 åœ¨ä»¥ä¸‹å¹³å°ä¸Šå¼€å‘ï¼š

* Jetson AGX Orin 64GBï¼ˆè¿è¡Œ Ubuntu 22.04 å’Œ JetPack 6.1ï¼‰
* Mac Studioï¼Œæ­è½½ Apple M2 Ultra å’Œ 48 GB ç»Ÿä¸€å†…å­˜ï¼ˆè¿è¡Œ MacOS Sequoiaï¼‰
* Mac Miniï¼Œæ­è½½ Apple M4 Pro å’Œ 48 GB ç»Ÿä¸€å†…å­˜ï¼ˆè¿è¡Œ MacOS Sequoiaï¼‰
* é€šç”¨ Linux æœºå™¨ï¼ˆè¿è¡Œ Ubuntu 22.04ï¼‰

OM1 åº”è¯¥å¯ä»¥åœ¨å…¶ä»–å¹³å°ï¼ˆå¦‚ Windowsï¼‰å’Œå¾®æ§åˆ¶å™¨ï¼ˆå¦‚ Raspberry Pi 5 16GBï¼‰ä¸Šè¿è¡Œã€‚

## å®Œå…¨è‡ªä¸»æ¨¡å¼æŒ‡å—

æˆ‘ä»¬å¾ˆé«˜å…´æ¨å‡º**å®Œå…¨è‡ªä¸»æ¨¡å¼**ï¼Œå…¶ä¸­ä¸‰ä¸ªæœåŠ¡åœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹å¾ªç¯ååŒå·¥ä½œï¼š

* **om1**
* **unitree_go2_ros2_sdk** â€“ ä¸€ä¸ª ROS 2 è½¯ä»¶åŒ…ï¼Œä½¿ç”¨ RPLiDAR ä¼ æ„Ÿå™¨ã€SLAM Toolbox å’Œ Nav2 æ ˆä¸º Unitree Go2 æœºå™¨äººæä¾› SLAMï¼ˆåŒæ­¥å®šä½ä¸åœ°å›¾æ„å»ºï¼‰èƒ½åŠ›ã€‚
* **om1-avatar** â€“ ä¸€ä¸ªç°ä»£çš„åŸºäº React çš„å‰ç«¯åº”ç”¨ç¨‹åºï¼Œä¸º OM1 æœºå™¨äººè½¯ä»¶æä¾›ç”¨æˆ·ç•Œé¢å’Œå¤´åƒæ˜¾ç¤ºç³»ç»Ÿã€‚

## Backpack ç®€ä»‹

ä»ç ”ç©¶åˆ°çœŸå®ä¸–ç•Œçš„è‡ªä¸»æ€§ï¼Œä¸€ä¸ªä¸æ‚¨ä¸€èµ·å­¦ä¹ ã€ç§»åŠ¨å’Œæ„å»ºçš„å¹³å°ã€‚æˆ‘ä»¬å°†å¾ˆå¿«å‘å¸ƒ**ç‰©æ–™æ¸…å•ï¼ˆBOMï¼‰**å’Œ**DIY** è¯¦æƒ…ã€‚æ•¬è¯·æœŸå¾…ï¼

å…‹éš†ä»¥ä¸‹ä»“åº“ï¼š

* https://github.com/OpenMind/OM1.git
* https://github.com/OpenMind/unitree_go2_ros2_sdk.git
* https://github.com/OpenMind/OM1-avatar.git

## å¯åŠ¨ç³»ç»Ÿ

è¦å¯åŠ¨æ‰€æœ‰æœåŠ¡ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

### OM1

è®¾ç½® API å¯†é’¥ï¼š

å¯¹äº Bashï¼š`vim ~/.bashrc` æˆ– `~/.bash_profile`ã€‚

å¯¹äº Zshï¼š`vim ~/.zshrc`ã€‚

æ·»åŠ ï¼š

```bash
export OM_API_KEY="your_api_key"
```

æ›´æ–° docker-compose æ–‡ä»¶ã€‚å°† "unitree_go2_autonomy_advance" æ›¿æ¢ä¸ºæ‚¨è¦è¿è¡Œçš„æ™ºèƒ½ä½“ï¼š

```yaml
command: ["unitree_go2_autonomy_advance"]
```

```bash
cd OM1
docker-compose up om1 -d --no-build
```

### unitree_go2_ros2_sdk

```bash
cd unitree_go2_ros2_sdk
docker-compose up orchestrator -d --no-build
docker-compose up om1_sensor -d --no-build
docker-compose up watchdog -d --no-build
```

### OM1-avatar

```bash
cd OM1-avatar
docker-compose up om1_avatar -d --no-build
```

## è¯¦ç»†æ–‡æ¡£

æ›´è¯¦ç»†çš„æ–‡æ¡£å¯ä»¥åœ¨ [docs.openmind.org](https://docs.openmind.org) è®¿é—®ã€‚

## è´¡çŒ®

åœ¨æäº¤ pull request ä¹‹å‰ï¼Œè¯·åŠ¡å¿…é˜…è¯»[è´¡çŒ®æŒ‡å—](CONTRIBUTING.md)ã€‚

## è®¸å¯è¯

æœ¬é¡¹ç›®æ ¹æ® [MIT è®¸å¯è¯](LICENSE)çš„æ¡æ¬¾æˆæƒï¼Œè¿™æ˜¯ä¸€ç§å®½æ¾çš„è‡ªç”±è½¯ä»¶è®¸å¯è¯ï¼Œå…è®¸ç”¨æˆ·è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹å’Œåˆ†å‘è½¯ä»¶ã€‚MIT è®¸å¯è¯æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨ä¸”æˆç†Ÿçš„è®¸å¯è¯ï¼Œä»¥å…¶ç®€å•æ€§å’Œçµæ´»æ€§è€Œé—»åã€‚é€šè¿‡ä½¿ç”¨ MIT è®¸å¯è¯ï¼Œæœ¬é¡¹ç›®æ—¨åœ¨é¼“åŠ±è½¯ä»¶çš„åä½œã€ä¿®æ”¹å’Œåˆ†å‘ã€‚

---

<a name="tiáº¿ng-viá»‡t"></a>

# OM1 - Há»‡ Thá»‘ng AI MÃ´-Ä‘un cho Robot

**[English](#english)** | **[ä¸­æ–‡](#ä¸­æ–‡)** | **Tiáº¿ng Viá»‡t**

![OM_Banner_X2](https://user-images.githubusercontent.com/placeholder/om_banner.png)

**[BÃ i bÃ¡o ká»¹ thuáº­t](https://openmind.org)** | **[TÃ i liá»‡u](https://docs.openmind.org)** | **[X/Twitter](https://x.com/openmind_agi)** | **[Discord](https://discord.gg/openmind)**

**OM1 cá»§a OpenMind lÃ  má»™t há»‡ thá»‘ng AI mÃ´-Ä‘un giÃºp cÃ¡c nhÃ  phÃ¡t triá»ƒn táº¡o vÃ  triá»ƒn khai cÃ¡c tÃ¡c nhÃ¢n AI Ä‘a phÆ°Æ¡ng thá»©c trÃªn mÃ´i trÆ°á»ng ká»¹ thuáº­t sá»‘ vÃ  robot váº­t lÃ½**, bao gá»“m Robot hÃ¬nh ngÆ°á»i, á»¨ng dá»¥ng Ä‘iá»‡n thoáº¡i, trang web, Robot bá»‘n chÃ¢n vÃ  robot giÃ¡o dá»¥c nhÆ° TurtleBot 4. CÃ¡c tÃ¡c nhÃ¢n OM1 cÃ³ thá»ƒ xá»­ lÃ½ nhiá»u Ä‘áº§u vÃ o Ä‘a dáº¡ng nhÆ° dá»¯ liá»‡u web, máº¡ng xÃ£ há»™i, nguá»“n cáº¥p camera vÃ  LIDAR, Ä‘á»“ng thá»i cho phÃ©p cÃ¡c hÃ nh Ä‘á»™ng váº­t lÃ½ bao gá»“m chuyá»ƒn Ä‘á»™ng, Ä‘iá»u hÆ°á»›ng tá»± Ä‘á»™ng vÃ  há»™i thoáº¡i tá»± nhiÃªn. Má»¥c tiÃªu cá»§a OM1 lÃ  lÃ m cho viá»‡c táº¡o ra cÃ¡c robot cÃ³ kháº£ nÄƒng cao, táº­p trung vÃ o con ngÆ°á»i trá»Ÿ nÃªn dá»… dÃ ng, dá»… nÃ¢ng cáº¥p vÃ  (tÃ¡i) cáº¥u hÃ¬nh Ä‘á»ƒ phÃ¹ há»£p vá»›i cÃ¡c hÃ¬nh thÃ¡i váº­t lÃ½ khÃ¡c nhau.

## Kháº£ nÄƒng cá»§a OM1

* **Kiáº¿n trÃºc mÃ´-Ä‘un**: ÄÆ°á»£c thiáº¿t káº¿ báº±ng Python Ä‘á»ƒ Ä‘Æ¡n giáº£n vÃ  tÃ­ch há»£p liá»n máº¡ch.
* **Äáº§u vÃ o dá»¯ liá»‡u**: Dá»… dÃ ng xá»­ lÃ½ dá»¯ liá»‡u vÃ  cáº£m biáº¿n má»›i.
* **Há»— trá»£ pháº§n cá»©ng qua Plugin**: Há»— trá»£ pháº§n cá»©ng má»›i thÃ´ng qua plugin cho cÃ¡c Ä‘iá»ƒm cuá»‘i API vÃ  káº¿t ná»‘i pháº§n cá»©ng robot cá»¥ thá»ƒ vá»›i `ROS2`, `Zenoh` vÃ  `CycloneDDS`. (ChÃºng tÃ´i khuyÃªn dÃ¹ng `Zenoh` cho táº¥t cáº£ phÃ¡t triá»ƒn má»›i).
* **MÃ n hÃ¬nh gá»¡ lá»—i dá»±a trÃªn Web**: GiÃ¡m sÃ¡t há»‡ thá»‘ng hoáº¡t Ä‘á»™ng vá»›i WebSim (cÃ³ sáºµn táº¡i http://localhost:8000/) Ä‘á»ƒ gá»¡ lá»—i trá»±c quan dá»… dÃ ng.
* **Äiá»ƒm cuá»‘i Ä‘Æ°á»£c cáº¥u hÃ¬nh sáºµn**: Há»— trá»£ Giá»ng nÃ³i sang VÄƒn báº£n, `gpt-4o` cá»§a OpenAI, DeepSeek vÃ  nhiá»u MÃ´ hÃ¬nh NgÃ´n ngá»¯ Thá»‹ giÃ¡c (VLM) vá»›i cÃ¡c Ä‘iá»ƒm cuá»‘i Ä‘Æ°á»£c cáº¥u hÃ¬nh sáºµn cho má»—i dá»‹ch vá»¥.

## Tá»•ng quan Kiáº¿n trÃºc

![Kiáº¿n trÃºc](https://user-images.githubusercontent.com/placeholder/architecture.png)

## Báº¯t Ä‘áº§u - Hello World

Äá»ƒ báº¯t Ä‘áº§u vá»›i OM1, hÃ£y cháº¡y tÃ¡c nhÃ¢n Spot. Spot sá»­ dá»¥ng webcam cá»§a báº¡n Ä‘á»ƒ chá»¥p vÃ  gáº¯n nhÃ£n cÃ¡c Ä‘á»‘i tÆ°á»£ng. CÃ¡c chÃº thÃ­ch vÄƒn báº£n nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c gá»­i Ä‘áº¿n `OpenAI 4o`, tráº£ vá» cÃ¡c lá»‡nh hÃ nh Ä‘á»™ng `chuyá»ƒn Ä‘á»™ng`, `lá»i nÃ³i` vÃ  `biá»ƒu cáº£m khuÃ´n máº·t`. CÃ¡c lá»‡nh nÃ y Ä‘Æ°á»£c hiá»ƒn thá»‹ trÃªn WebSim cÃ¹ng vá»›i thÃ´ng tin thá»i gian cÆ¡ báº£n vÃ  thÃ´ng tin gá»¡ lá»—i khÃ¡c.

### Quáº£n lÃ½ gÃ³i vÃ  mÃ´i trÆ°á»ng áº£o

Báº¡n sáº½ cáº§n trÃ¬nh quáº£n lÃ½ gÃ³i uv.

### Sao chÃ©p kho lÆ°u trá»¯

```bash
git clone https://github.com/openmind/OM1.git
cd OM1
git submodule update --init
uv venv
```

### CÃ i Ä‘áº·t cÃ¡c phá»¥ thuá»™c

**Cho MacOS:**

```bash
brew install portaudio ffmpeg
```

**Cho Linux:**

```bash
sudo apt-get update
sudo apt-get install portaudio19-dev python-dev ffmpeg
```

### Láº¥y KhÃ³a API OpenMind

Láº¥y KhÃ³a API cá»§a báº¡n táº¡i [OpenMind Portal](https://portal.openmind.org). Sao chÃ©p nÃ³ vÃ o `config/spot.json5`, thay tháº¿ pháº§n giá»¯ chá»— `openmind_free`. Hoáº·c, `cp env.example .env` vÃ  thÃªm khÃ³a cá»§a báº¡n vÃ o `.env`.

### Khá»Ÿi cháº¡y OM1

```bash
uv run src/run.py spot
```

Sau khi khá»Ÿi cháº¡y OM1, tÃ¡c nhÃ¢n Spot sáº½ tÆ°Æ¡ng tÃ¡c vá»›i báº¡n vÃ  thá»±c hiá»‡n cÃ¡c hÃ nh Ä‘á»™ng (mÃ´ phá»ng). Äá»ƒ Ä‘Æ°á»£c trá»£ giÃºp thÃªm vá» káº¿t ná»‘i OM1 vá»›i pháº§n cá»©ng robot cá»§a báº¡n, hÃ£y xem [hÆ°á»›ng dáº«n báº¯t Ä‘áº§u](https://docs.openmind.org/getting-started).

## Tiáº¿p theo lÃ  gÃ¬?

* Thá»­ má»™t sá»‘ [vÃ­ dá»¥](https://docs.openmind.org/examples)
* ThÃªm `Ä‘áº§u vÃ o` vÃ  `hÃ nh Ä‘á»™ng` má»›i.
* Thiáº¿t káº¿ cÃ¡c tÃ¡c nhÃ¢n vÃ  robot tÃ¹y chá»‰nh báº±ng cÃ¡ch táº¡o cÃ¡c tá»‡p cáº¥u hÃ¬nh `json5` cá»§a riÃªng báº¡n vá»›i cÃ¡c káº¿t há»£p tÃ¹y chá»‰nh cá»§a Ä‘áº§u vÃ o vÃ  hÃ nh Ä‘á»™ng.
* Thay Ä‘á»•i cÃ¡c lá»i nháº¯c há»‡ thá»‘ng trong cÃ¡c tá»‡p cáº¥u hÃ¬nh (náº±m trong `/config/`) Ä‘á»ƒ táº¡o cÃ¡c hÃ nh vi má»›i.

## Giao tiáº¿p vá»›i Pháº§n cá»©ng Robot Má»›i

OM1 giáº£ Ä‘á»‹nh ráº±ng pháº§n cá»©ng robot cung cáº¥p SDK cáº¥p cao cháº¥p nháº­n cÃ¡c lá»‡nh chuyá»ƒn Ä‘á»™ng vÃ  hÃ nh Ä‘á»™ng cÆ¡ báº£n nhÆ° `lá»™n nhÃ o`, `cháº¡y`, `nháº¹ nhÃ ng nháº·t quáº£ tÃ¡o Ä‘á»`, `move(0.37, 0, 0)` vÃ  `cÆ°á»i`. Má»™t vÃ­ dá»¥ Ä‘Æ°á»£c cung cáº¥p trong `actions/move_safe/connector/ros2.py`:

```python
...
elif output_interface.action == "shake paw":
    if self.sport_client:
        self.sport_client.Hello()
...
```

Náº¿u pháº§n cá»©ng robot cá»§a báº¡n chÆ°a cung cáº¥p HAL (lá»›p trá»«u tÆ°á»£ng pháº§n cá»©ng) phÃ¹ há»£p, cÃ¡c phÆ°Æ¡ng phÃ¡p robot truyá»n thá»‘ng nhÆ° RL (há»c tÄƒng cÆ°á»ng) káº¿t há»£p vá»›i mÃ´i trÆ°á»ng mÃ´ phá»ng phÃ¹ há»£p (Unity, Gazebo), cáº£m biáº¿n (nhÆ° camera Ä‘á»™ sÃ¢u ZED gáº¯n tay) vÃ  VLA tÃ¹y chá»‰nh sáº½ cáº§n thiáº¿t Ä‘á»ƒ báº¡n táº¡o ra má»™t. NgoÃ i ra, giáº£ Ä‘á»‹nh ráº±ng HAL cá»§a báº¡n cháº¥p nháº­n quá»¹ Ä‘áº¡o chuyá»ƒn Ä‘á»™ng, cung cáº¥p quáº£n lÃ½/giÃ¡m sÃ¡t pin vÃ  nhiá»‡t, vÃ  hiá»‡u chá»‰nh vÃ  Ä‘iá»u chá»‰nh cÃ¡c cáº£m biáº¿n nhÆ° IMU, LIDAR vÃ  tá»« káº¿.

OM1 cÃ³ thá»ƒ giao tiáº¿p vá»›i HAL cá»§a báº¡n qua USB, serial, ROS2, CycloneDDS, Zenoh hoáº·c websocket. Äá»ƒ xem vÃ­ dá»¥ vá» HAL robot hÃ¬nh ngÆ°á»i nÃ¢ng cao, vui lÃ²ng xem [C++ SDK cá»§a Unitree](https://github.com/unitreerobotics). ThÆ°á»ng xuyÃªn, HAL, Ä‘áº·c biá»‡t lÃ  mÃ£ ROS2, sáº½ Ä‘Æ°á»£c docker hÃ³a vÃ  sau Ä‘Ã³ cÃ³ thá»ƒ giao tiáº¿p vá»›i OM1 thÃ´ng qua middleware DDS hoáº·c websocket.

## Ná»n táº£ng PhÃ¡t triá»ƒn ÄÆ°á»£c Ä‘á» xuáº¥t

OM1 Ä‘Æ°á»£c phÃ¡t triá»ƒn trÃªn:

* Jetson AGX Orin 64GB (cháº¡y Ubuntu 22.04 vÃ  JetPack 6.1)
* Mac Studio vá»›i Apple M2 Ultra vá»›i bá»™ nhá»› thá»‘ng nháº¥t 48 GB (cháº¡y MacOS Sequoia)
* Mac Mini vá»›i Apple M4 Pro vá»›i bá»™ nhá»› thá»‘ng nháº¥t 48 GB (cháº¡y MacOS Sequoia)
* MÃ¡y Linux chung (cháº¡y Ubuntu 22.04)

OM1 _nÃªn_ cháº¡y trÃªn cÃ¡c ná»n táº£ng khÃ¡c (nhÆ° Windows) vÃ  vi Ä‘iá»u khiá»ƒn nhÆ° Raspberry Pi 5 16GB.

## HÆ°á»›ng dáº«n Tá»± Ä‘á»™ng HoÃ n toÃ n

ChÃºng tÃ´i ráº¥t vui má»«ng giá»›i thiá»‡u **cháº¿ Ä‘á»™ tá»± Ä‘á»™ng hoÃ n toÃ n**, trong Ä‘Ã³ ba dá»‹ch vá»¥ hoáº¡t Ä‘á»™ng cÃ¹ng nhau trong má»™t vÃ²ng láº·p mÃ  khÃ´ng cáº§n can thiá»‡p thá»§ cÃ´ng:

* **om1**
* **unitree_go2_ros2_sdk** â€“ Má»™t gÃ³i ROS 2 cung cáº¥p kháº£ nÄƒng SLAM (Äá»‹nh vá»‹ vÃ  Láº­p báº£n Ä‘á»“ Äá»“ng thá»i) cho robot Unitree Go2 sá»­ dá»¥ng cáº£m biáº¿n RPLiDAR, SLAM Toolbox vÃ  ngÄƒn xáº¿p Nav2.
* **om1-avatar** â€“ Má»™t á»©ng dá»¥ng frontend hiá»‡n Ä‘áº¡i dá»±a trÃªn React cung cáº¥p giao diá»‡n ngÆ°á»i dÃ¹ng vÃ  há»‡ thá»‘ng hiá»ƒn thá»‹ avatar cho pháº§n má»m robot OM1.

## Giá»›i thiá»‡u vá» Backpack

Tá»« nghiÃªn cá»©u Ä‘áº¿n tá»± Ä‘á»™ng trong tháº¿ giá»›i thá»±c, má»™t ná»n táº£ng há»c há»i, di chuyá»ƒn vÃ  xÃ¢y dá»±ng cÃ¹ng báº¡n. ChÃºng tÃ´i sáº½ sá»›m phÃ¡t hÃ nh **BOM** vÃ  chi tiáº¿t vá» **DIY** cho nÃ³. HÃ£y theo dÃµi!

Sao chÃ©p cÃ¡c kho lÆ°u trá»¯ sau:

* https://github.com/OpenMind/OM1.git
* https://github.com/OpenMind/unitree_go2_ros2_sdk.git
* https://github.com/OpenMind/OM1-avatar.git

## Khá»Ÿi Ä‘á»™ng Há»‡ thá»‘ng

Äá»ƒ khá»Ÿi Ä‘á»™ng táº¥t cáº£ cÃ¡c dá»‹ch vá»¥, hÃ£y cháº¡y cÃ¡c lá»‡nh sau:

### Cho OM1

Thiáº¿t láº­p khÃ³a API:

Cho Bash: `vim ~/.bashrc` hoáº·c `~/.bash_profile`.

Cho Zsh: `vim ~/.zshrc`.

ThÃªm:

```bash
export OM_API_KEY="your_api_key"
```

Cáº­p nháº­t tá»‡p docker-compose. Thay tháº¿ "unitree_go2_autonomy_advance" báº±ng tÃ¡c nhÃ¢n báº¡n muá»‘n cháº¡y:

```yaml
command: ["unitree_go2_autonomy_advance"]
```

```bash
cd OM1
docker-compose up om1 -d --no-build
```

### Cho unitree_go2_ros2_sdk

```bash
cd unitree_go2_ros2_sdk
docker-compose up orchestrator -d --no-build
docker-compose up om1_sensor -d --no-build
docker-compose up watchdog -d --no-build
```

### Cho OM1-avatar

```bash
cd OM1-avatar
docker-compose up om1_avatar -d --no-build
```

## TÃ i liá»‡u Chi tiáº¿t

TÃ i liá»‡u chi tiáº¿t hÆ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c truy cáº­p táº¡i [docs.openmind.org](https://docs.openmind.org).

## ÄÃ³ng gÃ³p

Vui lÃ²ng Ä‘áº£m báº£o Ä‘á»c [HÆ°á»›ng dáº«n ÄÃ³ng gÃ³p](CONTRIBUTING.md) trÆ°á»›c khi thá»±c hiá»‡n pull request.

## Giáº¥y phÃ©p

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c cáº¥p phÃ©p theo cÃ¡c Ä‘iá»u khoáº£n cá»§a [Giáº¥y phÃ©p MIT](LICENSE), Ä‘Ã¢y lÃ  giáº¥y phÃ©p pháº§n má»m tá»± do cho phÃ©p ngÆ°á»i dÃ¹ng tá»± do sá»­ dá»¥ng, sá»­a Ä‘á»•i vÃ  phÃ¢n phá»‘i pháº§n má»m. Giáº¥y phÃ©p MIT lÃ  má»™t giáº¥y phÃ©p Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i vÃ  Ä‘Æ°á»£c thiáº¿t láº­p tá»‘t, ná»•i tiáº¿ng vÃ¬ tÃ­nh Ä‘Æ¡n giáº£n vÃ  linh hoáº¡t cá»§a nÃ³. Báº±ng cÃ¡ch sá»­ dá»¥ng Giáº¥y phÃ©p MIT, dá»± Ã¡n nÃ y nháº±m khuyáº¿n khÃ­ch sá»± há»£p tÃ¡c, sá»­a Ä‘á»•i vÃ  phÃ¢n phá»‘i pháº§n má»m.

---

<div align="center">

**Made with â¤ï¸ by the OpenMind Team**

[â­ Star us on GitHub](https://github.com/OpenMind/OM1) | [ğŸ› Report Issues](https://github.com/OpenMind/OM1/issues) | [ğŸ’¬ Join our Community](https://discord.gg/openmind)

</div>

